{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4f95a5",
   "metadata": {},
   "source": [
    "# 2 layer Neural network from Scratch\n",
    "https://youtu.be/vcZub77WvFA?list=PL2-dafEMk2A5BoX3KyKu6ti5_Pytp91sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b78cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f720f2",
   "metadata": {},
   "source": [
    "Define <b>variables</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e56836b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us define our variables\n",
    "n_hidden = 10 # number of hidden neurons\n",
    "n_in = 10\n",
    "\n",
    "n_out = 10 # number of outputs\n",
    "n_sample = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c177707",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e19fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "np.random.seed(0) # we want to make sure that every time we run this code, the same random number is generated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9687b",
   "metadata": {},
   "source": [
    "Define our <b>activation function</b> \n",
    "https://youtu.be/-7scQpJT7uo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868aedf0",
   "metadata": {},
   "source": [
    "Activation functions add nonlinearity properties to our network. A <b>Linear function</b> is a polynomial with just one degree,i.e <b> y = x or y = 2x</b> this always forms a straight line. But every other equation is <b> non-linear</b> . Linear equations are easy to solve but they are limited in their complexity and neural networks are known as <b> universal function approximators</b>. Any process you can imagine can be thought of as a function computation, so we need a way to compute not just linear fxn but non-linear fxn as well "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1999a7",
   "metadata": {},
   "source": [
    "Popular ones are: <b>sigmoid function</b> - takes some number and squashes it into a range between 0 - 1. The problem is it causes are gradient to vanish (<b>vanishing and exploding gradient</b>):\n",
    "- When a neuron's activation saturates it close to either 0 or 1, the gradient at these regions is very close to zero. During back propagation this local gradient will be multiplied by this gates output for the whole objective, so if the local gradient is really small it will make the gradient slowly vanish and close to no signal will flow through the neuron to it's weights and recursively to its data.\n",
    "- Second problem is that it's output isn't <b>centered</b>. It starts from 0 and ends at 1. That means the value after the function will be positive and that makes the gradient of the weights becomes either all negative or all positive. This makes the gradient updates go too far in different directions which make optimazation harder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5cede3",
   "metadata": {},
   "source": [
    "We also have <b>tanh(x)</b>, its squashes the real numbers into into a range -1 to 1 so its output is zero centered which makes optimization easier. In practice it is preferred over the sigmoid. But just like the sigmoid it suffers from the <b>vanishing gradient problem</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03238ecc",
   "metadata": {},
   "source": [
    "So then we have <b>ReLU</b>, which has become very popular. It's simply <b>max 0x</b> - which means that the value is 0 when x is less than 0 and linear with a slope of 1 when x is greater than 0. It was noted that it had a 6x convergence over tanh(x) in the landmark imagenet classification paper by Khrushchevski. It doesn't involve expensive calculations like tanh(x) or sigmoid so it learns faster and it avoids the vanishing gradient problem. <i>But it's used for the <b>hidden layers</b></i>.\n",
    "The output layer should use a <b>Soft max function for classification since it gives probabilities for different classes</b> and a <b>linear function for Regression</b> since the signal goes through unchanged.\n",
    "<b>NB:</b> One problem that ReLU has is that some units can become fragileduring training and die; meaning a big gradient flowing through a neuron could cause a weight update that makes it never activate on any data point again, so then gradients flowing through it will always be zero from that point on. So a varient was introduced callled the <b>Leaky ReLU</b> to slove this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98198a",
   "metadata": {},
   "source": [
    "<img src=\"activation-functions.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad250d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "# Hyperbolic tangent function\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685471f",
   "metadata": {},
   "source": [
    "<b>Training function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e96246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains 5 parameters\n",
    "# x - input data\n",
    "# t - transpose, which is going to help in matrix multiplication\n",
    "# V, W - layers\n",
    "# bv, bw - biasis\n",
    "\n",
    "def train(x, t, V, W, bv, bw):\n",
    "    \n",
    "    # forward propagation. Matrix multiply + biasis\n",
    "    A = np.dot(x,V) + bv # multiply the inputs by the weights and add the bias\n",
    "    Z = np.tanh(A) # apply an activation function. Repeat for next layer\n",
    "    \n",
    "    B = np.dot(Z, W) + bw\n",
    "    Y = sigmoid(B)\n",
    "    \n",
    "    # Backward propagation\n",
    "    Ew = Y - t # substracting the output of th last layer with its inverse\n",
    "    Ev = tanh_prime(A) * np.dot(W, Ew)\n",
    "    \n",
    "    # Predict loss\n",
    "    dW = np.outer(Z, Ew) \n",
    "    dV = np.outer(x, Ev)\n",
    "    \n",
    "    # loss function     -- cross entropy, generally prefered for 'classification'\n",
    "    loss = -np.mean(t * np.log(Y) + (1 - t) * np.log(1 - Y ))\n",
    "    return loss, (dV, dW, Ev, Ew)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09d165",
   "metadata": {},
   "source": [
    "<img src=\"ml-model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5954ce0b",
   "metadata": {},
   "source": [
    "The final output value is our prediction. We find the difference between it and the expected output, then use that error value to compute the partial derivative with respect to the weights in each layer going backwards recursively. We then update the wieghts with these values and repeate the proceess until the error is as small as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b19d2",
   "metadata": {},
   "source": [
    "<b>Prediction function</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577c2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, V, W, bv, bw):\n",
    "    A = np.dot(x, V) + bv\n",
    "    B = np.dot(np.tanh(A), W) + bw\n",
    "    return (sigmoid(B) > 0.5).astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25827846",
   "metadata": {},
   "source": [
    "Creating layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83bce8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "range(0, 300)\n"
     ]
    }
   ],
   "source": [
    "#n_hidden = 10 # number of hidden neurons\n",
    "#n_in = 10\n",
    "\n",
    "#n_out = 10 # number of outputs\n",
    "#n_sample = 300\n",
    "\n",
    "V = np.random.normal(scale = 0.1, size = (n_in, n_hidden))\n",
    "W = np.random.normal(scale = 0.1, size = ( n_hidden, n_out))\n",
    "\n",
    "# initialize our biasis\n",
    "bv = np.zeros(n_hidden)\n",
    "bw = np.zeros(n_out)\n",
    "\n",
    "params = [V, W, bv, bw]\n",
    "\n",
    "# Generating our data using numpy\n",
    "X = np.random.binomial(1, 0.5, (n_sample, n_in))\n",
    "T = X ^ 1 # the inverse of X\n",
    "\n",
    "#print(X[5])  [0 0 1 1 0 1 0 0 0 0]\n",
    "#print(T[5])  [1 1 0 0 1 0 1 1 1 1]\n",
    "print(len(params))\n",
    "print(range(X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f775b9e",
   "metadata": {},
   "source": [
    "<b>Training time</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842ba081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, loss: 0.45465070, Time: 0.0296s\n",
      " Epoch: 1, loss: 0.13697961, Time: 0.0275s\n",
      " Epoch: 2, loss: 0.06206941, Time: 0.0316s\n",
      " Epoch: 3, loss: 0.04092746, Time: 0.0175s\n",
      " Epoch: 4, loss: 0.03159958, Time: 0.0186s\n",
      " Epoch: 5, loss: 0.02592744, Time: 0.0236s\n",
      " Epoch: 6, loss: 0.02199575, Time: 0.0243s\n",
      " Epoch: 7, loss: 0.01907812, Time: 0.0186s\n",
      " Epoch: 8, loss: 0.01682099, Time: 0.0185s\n",
      " Epoch: 9, loss: 0.01502363, Time: 0.0213s\n",
      " Epoch: 10, loss: 0.01356039, Time: 0.0199s\n",
      " Epoch: 11, loss: 0.01234775, Time: 0.0219s\n",
      " Epoch: 12, loss: 0.01132776, Time: 0.0234s\n",
      " Epoch: 13, loss: 0.01045887, Time: 0.0243s\n",
      " Epoch: 14, loss: 0.00971052, Time: 0.0244s\n",
      " Epoch: 15, loss: 0.00905971, Time: 0.0200s\n",
      " Epoch: 16, loss: 0.00848887, Time: 0.0241s\n",
      " Epoch: 17, loss: 0.00798436, Time: 0.0242s\n",
      " Epoch: 18, loss: 0.00753542, Time: 0.0152s\n",
      " Epoch: 19, loss: 0.00713347, Time: 0.0166s\n",
      " Epoch: 20, loss: 0.00677160, Time: 0.0235s\n",
      " Epoch: 21, loss: 0.00644415, Time: 0.0227s\n",
      " Epoch: 22, loss: 0.00614650, Time: 0.0232s\n",
      " Epoch: 23, loss: 0.00587477, Time: 0.0221s\n",
      " Epoch: 24, loss: 0.00562576, Time: 0.0239s\n",
      " Epoch: 25, loss: 0.00539674, Time: 0.0244s\n",
      " Epoch: 26, loss: 0.00518541, Time: 0.0259s\n",
      " Epoch: 27, loss: 0.00498981, Time: 0.0276s\n",
      " Epoch: 28, loss: 0.00480824, Time: 0.0171s\n",
      " Epoch: 29, loss: 0.00463926, Time: 0.0238s\n",
      " Epoch: 30, loss: 0.00448161, Time: 0.0225s\n",
      " Epoch: 31, loss: 0.00433419, Time: 0.0230s\n",
      " Epoch: 32, loss: 0.00419603, Time: 0.0243s\n",
      " Epoch: 33, loss: 0.00406629, Time: 0.0247s\n",
      " Epoch: 34, loss: 0.00394423, Time: 0.0239s\n",
      " Epoch: 35, loss: 0.00382919, Time: 0.0241s\n",
      " Epoch: 36, loss: 0.00372058, Time: 0.0232s\n",
      " Epoch: 37, loss: 0.00361787, Time: 0.0242s\n",
      " Epoch: 38, loss: 0.00352061, Time: 0.0241s\n",
      " Epoch: 39, loss: 0.00342836, Time: 0.0236s\n",
      " Epoch: 40, loss: 0.00334076, Time: 0.0279s\n",
      " Epoch: 41, loss: 0.00325746, Time: 0.0158s\n",
      " Epoch: 42, loss: 0.00317815, Time: 0.0188s\n",
      " Epoch: 43, loss: 0.00310255, Time: 0.0253s\n",
      " Epoch: 44, loss: 0.00303042, Time: 0.0178s\n",
      " Epoch: 45, loss: 0.00296151, Time: 0.0172s\n",
      " Epoch: 46, loss: 0.00289562, Time: 0.0238s\n",
      " Epoch: 47, loss: 0.00283255, Time: 0.0184s\n",
      " Epoch: 48, loss: 0.00277213, Time: 0.0249s\n",
      " Epoch: 49, loss: 0.00271419, Time: 0.0253s\n",
      " Epoch: 50, loss: 0.00265858, Time: 0.0251s\n",
      " Epoch: 51, loss: 0.00260517, Time: 0.0308s\n",
      " Epoch: 52, loss: 0.00255383, Time: 0.0331s\n",
      " Epoch: 53, loss: 0.00250444, Time: 0.0274s\n",
      " Epoch: 54, loss: 0.00245689, Time: 0.0279s\n",
      " Epoch: 55, loss: 0.00241108, Time: 0.0272s\n",
      " Epoch: 56, loss: 0.00236692, Time: 0.0236s\n",
      " Epoch: 57, loss: 0.00232432, Time: 0.0263s\n",
      " Epoch: 58, loss: 0.00228320, Time: 0.0233s\n",
      " Epoch: 59, loss: 0.00224348, Time: 0.0236s\n",
      " Epoch: 60, loss: 0.00220510, Time: 0.0338s\n",
      " Epoch: 61, loss: 0.00216798, Time: 0.0267s\n",
      " Epoch: 62, loss: 0.00213207, Time: 0.0235s\n",
      " Epoch: 63, loss: 0.00209730, Time: 0.0232s\n",
      " Epoch: 64, loss: 0.00206363, Time: 0.0208s\n",
      " Epoch: 65, loss: 0.00203101, Time: 0.0251s\n",
      " Epoch: 66, loss: 0.00199938, Time: 0.0213s\n",
      " Epoch: 67, loss: 0.00196870, Time: 0.0194s\n",
      " Epoch: 68, loss: 0.00193892, Time: 0.0196s\n",
      " Epoch: 69, loss: 0.00191002, Time: 0.0205s\n",
      " Epoch: 70, loss: 0.00188195, Time: 0.0322s\n",
      " Epoch: 71, loss: 0.00185467, Time: 0.0307s\n",
      " Epoch: 72, loss: 0.00182816, Time: 0.0214s\n",
      " Epoch: 73, loss: 0.00180238, Time: 0.0241s\n",
      " Epoch: 74, loss: 0.00177730, Time: 0.0229s\n",
      " Epoch: 75, loss: 0.00175290, Time: 0.0245s\n",
      " Epoch: 76, loss: 0.00172914, Time: 0.0217s\n",
      " Epoch: 77, loss: 0.00170600, Time: 0.0244s\n",
      " Epoch: 78, loss: 0.00168346, Time: 0.0230s\n",
      " Epoch: 79, loss: 0.00166149, Time: 0.0263s\n",
      " Epoch: 80, loss: 0.00164008, Time: 0.0334s\n",
      " Epoch: 81, loss: 0.00161920, Time: 0.0313s\n",
      " Epoch: 82, loss: 0.00159883, Time: 0.0331s\n",
      " Epoch: 83, loss: 0.00157896, Time: 0.0241s\n",
      " Epoch: 84, loss: 0.00155957, Time: 0.0254s\n",
      " Epoch: 85, loss: 0.00154063, Time: 0.0277s\n",
      " Epoch: 86, loss: 0.00152214, Time: 0.0261s\n",
      " Epoch: 87, loss: 0.00150407, Time: 0.0221s\n",
      " Epoch: 88, loss: 0.00148642, Time: 0.0251s\n",
      " Epoch: 89, loss: 0.00146917, Time: 0.0200s\n",
      " Epoch: 90, loss: 0.00145230, Time: 0.0273s\n",
      " Epoch: 91, loss: 0.00143581, Time: 0.0244s\n",
      " Epoch: 92, loss: 0.00141968, Time: 0.0290s\n",
      " Epoch: 93, loss: 0.00140390, Time: 0.0317s\n",
      " Epoch: 94, loss: 0.00138846, Time: 0.0312s\n",
      " Epoch: 95, loss: 0.00137334, Time: 0.0277s\n",
      " Epoch: 96, loss: 0.00135854, Time: 0.0247s\n",
      " Epoch: 97, loss: 0.00134405, Time: 0.0222s\n",
      " Epoch: 98, loss: 0.00132986, Time: 0.0233s\n",
      " Epoch: 99, loss: 0.00131596, Time: 0.0322s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    err = [] # error list\n",
    "    upd = [0]*len(params)\n",
    "    \n",
    "    t0 = time.process_time()\n",
    "    # for each data point, update our weights\n",
    "    for i in range(X.shape[0]): # we use the .shape() fxn to know how big our data is\n",
    "        loss, grad = train(X[i], T[i], *params) # bcos we don't want to write V,W,bv,... we just use '*params'\n",
    "        \n",
    "        # update loss\n",
    "        for j in range(len(params)):\n",
    "            params[j] -= upd[j]\n",
    "        \n",
    "        for j in range(len(params)):\n",
    "            upd[j] = learning_rate * grad[j] + momentum * upd[j] \n",
    "            \n",
    "        err.append(loss)\n",
    "        \n",
    "    print(\" Epoch: %d, loss: %.8f, Time: %.4fs\" %(epoch, np.mean(err), time.process_time() - t0))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576838df",
   "metadata": {},
   "source": [
    "Time to predict something "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3629faaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Prediction\n",
      "[1 0 1 1 1 1 0 1 0 0]\n",
      "[0 1 0 0 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.binomial(1 ,0.5, n_in)\n",
    "print(\"XOR Prediction\")\n",
    "print(x)\n",
    "print(predict(x, *params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
